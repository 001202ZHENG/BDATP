{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "ein.tags": [
          "worksheet-0"
        ],
        "id": "eOeG1er1Sjum"
      },
      "source": [
        "<h6><center>Big Data Algorithms Techniques & Platforms</center></h6>\n",
        "\n",
        "<h1>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "<center>Assignment 1: Introduction to MapReduce</center>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "</h1>\n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "For this assignment, as you did in the first lab, **do not use**:\n",
        "\n",
        "\n",
        "\n",
        "*   the native fuctions <code>map()</code> and <code>reduce()</code> of python\n",
        "*   the Pandas dataframes\n",
        "*   the PySpark dataframes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2romc2VZJTCG"
      },
      "source": [
        "# Setting the environment\n",
        "\n",
        "[Kaggle](https://www.kaggle.com/) is a data science competition platform and online community of data scientists and machine learning practitioners under Google LLC. In Kaggle you can also find and publish datasets. For our exercises we will often use datasets published on Kaggle.\n",
        "\n",
        "## A. Environment set-up\n",
        "\n",
        "It is possible to folllow the following procedure that includes directly Kaggle data into colab working folders.\n",
        "\n",
        "### Step 1: Download the token from Kaggle\n",
        "\n",
        "To download data from Kaggle directly in your Colab environment you need to follow these steps:\n",
        "\n",
        "1.   Authenticate with the Kaggle services (if you do not have an account you must create one)\n",
        "2.   Get your API token.\n",
        "\n",
        "The token can be easily generated and downloaded from the profile section of your Kaggle account.\n",
        "\n",
        "Simply, navigate to your Kaggle profile and then, **click** the **Account** tab and then scroll down to the API section.\n",
        "\n",
        "A file named <code>kaggle.json</code> will be download.\n",
        "\n",
        "This file contains the username and the API key.\n",
        "\n",
        "This is a one-time step and you don’t need to generate the credentials every time you download the dataset.\n",
        "\n",
        "\n",
        "Upload the file <code>kaggle.json</code> that you just downloaded from Kaggle in the data folder of your Colab environment and execute the following commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jflw_zUvKYUM",
        "outputId": "7f518699-20b6-4a9c-b8e3-add0ffbbcca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (1.5.16)\n",
            "Requirement already satisfied: python-dateutil in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: requests in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (2.24.0)\n",
            "Requirement already satisfied: urllib3 in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (1.25.11)\n",
            "Requirement already satisfied: tqdm in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: python-slugify in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: certifi in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (2020.6.20)\n",
            "Requirement already satisfied: bleach in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (3.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: webencodings in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: packaging in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from bleach->kaggle) (20.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages (from packaging->bleach->kaggle) (2.4.7)\n",
            "mkdir: /Users/zhengwan/.kaggle: File exists\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzbwjTBHKfXE"
      },
      "source": [
        "## B. Importing data\n",
        "\n",
        "Execute the following line to import data, we will discuss the database format in the following paragraphs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwyBdXRwLph4",
        "outputId": "ba9b0683-1bbd-4861-9639-e4078f7be4c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading sms-spam-collection-dataset.zip to /Users/zhengwan/Desktop/BDATP/ass_1\n",
            "100%|████████████████████████████████████████| 211k/211k [00:00<00:00, 1.37MB/s]\n",
            "100%|████████████████████████████████████████| 211k/211k [00:00<00:00, 1.36MB/s]\n"
          ]
        }
      ],
      "source": [
        "! kaggle datasets download uciml/sms-spam-collection-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VN4DeqbMSuF",
        "outputId": "4f167af9-d163-45e5-fb0c-2349fbee4491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  sms-spam-collection-dataset.zip\n",
            "replace spam.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
          ]
        }
      ],
      "source": [
        "! unzip sms-spam-collection-dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk34RU8JSjuz"
      },
      "source": [
        "# Exercise: Messages and statistics\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Suppose now that you have a text file generated by a monitoring system of a company.\n",
        "\n",
        "Each line represents a tagged message that have been collected from SMS Spam research.\n",
        "\n",
        "Each message can me tagged as ham (legitimate) or spam.\n",
        "\n",
        "The items are separated by a comma.\n",
        "\n",
        "A sample line:\n",
        "\n",
        "`ham,There are many company. Tell me the language.,,,`\n",
        "\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP2tGeojcxKM"
      },
      "source": [
        "## Part 1 - Count Spam - Ham\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Describe and implement a Map-Reduce algorithm that provides the total number of spam messages and ham messages:\n",
        "\n",
        "```\n",
        "Ham: ...\n",
        "Spam: ...\n",
        "```\n",
        "\n",
        "\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "<p>\n",
        "\n",
        "**Hint**:\n",
        "\n",
        "Read the .csv file line by line and think about how to consider the keys.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49mYn6x7du2D",
        "outputId": "727f8641-7d55-4044-dde5-3a5e527f3a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['v1', 'v2', '', '', ''], ['ham', 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', '', '', ''], ['ham', 'Ok lar... Joking wif u oni...', '', '', ''], ['spam', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", '', '', ''], ['ham', 'U dun say so early hor... U c already then say...', '', '', '']]\n"
          ]
        }
      ],
      "source": [
        "# Write here any support function you need to read the .csv file and/or format your text\n",
        "import csv\n",
        "\n",
        "def read_csv_file(file_path):\n",
        "    \"\"\"\n",
        "    Read CSV file and return data\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_path\n",
        "\n",
        "    Return\n",
        "    ----------\n",
        "    list\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='ISO-8859-1') as csv_file:\n",
        "      csv_reader = csv.reader(csv_file)\n",
        "      for row in csv_reader:\n",
        "        data.append(row)\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "data = read_csv_file('spam.csv')\n",
        "\n",
        "print (data[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2j1gZjMRudu"
      },
      "source": [
        "### <strong> 1.1. Map </strong>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ved9CtvMSju0",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('ham', 1),\n",
              " ('ham', 1),\n",
              " ('spam', 1),\n",
              " ('ham', 1),\n",
              " ('ham', 1),\n",
              " ('spam', 1),\n",
              " ('ham', 1),\n",
              " ('ham', 1),\n",
              " ('spam', 1),\n",
              " ('spam', 1),\n",
              " ('ham', 1),\n",
              " ('spam', 1),\n",
              " ('spam', 1),\n",
              " ('ham', 1),\n",
              " ('ham', 1)]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Provide here few comment lines that describe your map function\n",
        "\n",
        "def  map_num_spam_ham(data):\n",
        "  '''\n",
        "  This function takes a dataset as input and maps the first element of each data row (typically \"ham\" or \"spam\") to a key-value pair,\n",
        "  where the key is that element, and the value is 1.\n",
        "\n",
        "  param\n",
        "  -----\n",
        "  data: Input dataset, typically a list or RDD containing multiple rows of data.\n",
        "\n",
        "  return: Returns a list containing key-value pairs, where the key is the first element of the data, and the value is 1.\n",
        "  '''\n",
        "\n",
        "  res =[]\n",
        "  for i in data[1:]:\n",
        "    value = i[0]\n",
        "    res.append((value,1))\n",
        "  return res\n",
        "\n",
        "\n",
        "res1 = map_num_spam_ham(data)\n",
        "res1[:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjMddtoxck4_"
      },
      "source": [
        "### <strong> 1.2. Shuffle </strong>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y38PCNFGRpQH"
      },
      "outputs": [],
      "source": [
        "#Provide here few comment lines that describe your shuffle function\n",
        "from collections import defaultdict\n",
        "\n",
        "def  shuffle_num_spam_ham(input):\n",
        "  '''\n",
        "  This function takes a list of key-value pairs as input and performs a shuffle operation to group the values by their corresponding keys.\n",
        "\n",
        "  param\n",
        "  -----\n",
        "  input: A list of key-value pairs to be shuffled.\n",
        "  return: Returns a dictionary where keys are unique keys from the input, and values are lists of associated values.\n",
        "  '''\n",
        "  dict= defaultdict(list)\n",
        "  for key,value in input:\n",
        "    dict[key].append(value)\n",
        "  return dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "res2 = shuffle_num_spam_ham(res1)\n",
        "res2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xef3_OtScnXe"
      },
      "source": [
        "### <strong> 1.3. Reduce </strong>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_ZSp0GuRshs",
        "outputId": "4c4bae7d-7bd0-4f4e-ea35-3bf41fc5ff5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('ham', 4825), ('spam', 747)]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Provide here few comment lines that describe your reduce function\n",
        "\n",
        "def  reduce_num_spam_ham(input):\n",
        "    '''\n",
        "    This function takes a dictionary and reduces the data by counting the number of elements in each list and returns a list of key-value pairs\n",
        "    :param input: A dictionary where keys represent categories or values, and values are lists of associated data.\n",
        "    :return: Returns a list of key-value pairs, where each key is a category or value, and the value is the count of associated elements.\n",
        "\n",
        "    '''\n",
        "    res = []\n",
        "    for key in input.keys():\n",
        "      res.append((key,len(input[key])))\n",
        "    return res\n",
        "\n",
        "\n",
        "res3 = reduce_num_spam_ham(res2)\n",
        "res3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe-FkbgCSju2",
        "scrolled": true
      },
      "source": [
        "\n",
        "## Part 2 - Number of word in ham and spam messages\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Describe and implement an algorithm that, using the Map-Reduce paradigm, lists the total number of word in spam messages and the total number of word in ham messages.\n",
        "\n",
        "You can decide if removing stop words is significant or not for this analysis.\n",
        "</font>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLH2jDDpdCAm"
      },
      "source": [
        "### <strong> 2.1. Map </strong>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gnHyNaXzSju2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('spam',\n",
              "  ['free',\n",
              "   'entry',\n",
              "   'in',\n",
              "   '2',\n",
              "   'a',\n",
              "   'wkly',\n",
              "   'comp',\n",
              "   'to',\n",
              "   'win',\n",
              "   'fa',\n",
              "   'cup',\n",
              "   'final',\n",
              "   'tkts',\n",
              "   '21st',\n",
              "   'may',\n",
              "   '2005',\n",
              "   'text',\n",
              "   'fa',\n",
              "   'to',\n",
              "   '87121',\n",
              "   'to',\n",
              "   'receive',\n",
              "   'entry',\n",
              "   'questionstd',\n",
              "   'txt',\n",
              "   'ratetcs',\n",
              "   'apply',\n",
              "   '08452810075over18s']),\n",
              " ('spam',\n",
              "  ['freemsg',\n",
              "   'hey',\n",
              "   'there',\n",
              "   'darling',\n",
              "   'its',\n",
              "   'been',\n",
              "   '3',\n",
              "   'weeks',\n",
              "   'now',\n",
              "   'and',\n",
              "   'no',\n",
              "   'word',\n",
              "   'back',\n",
              "   'id',\n",
              "   'like',\n",
              "   'some',\n",
              "   'fun',\n",
              "   'you',\n",
              "   'up',\n",
              "   'for',\n",
              "   'it',\n",
              "   'still',\n",
              "   'tb',\n",
              "   'ok',\n",
              "   'xxx',\n",
              "   'std',\n",
              "   'chgs',\n",
              "   'to',\n",
              "   'send',\n",
              "   'å£150',\n",
              "   'to',\n",
              "   'rcv'])]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Provide here few comment lines that describe your map function\n",
        "import string\n",
        "\n",
        "def  map_words_in_msg(input):\n",
        "  \"\"\"\n",
        "Maps input data to lowercase, removes punctuation, and splits text into words.\n",
        "\n",
        "Args:\n",
        "    input (list of tuples): Input data where each tuple contains (label, text).\n",
        "\n",
        "Returns:\n",
        "    list of tuples: Each tuple contains a label and a list of words.\n",
        "\"\"\"\n",
        "  res_1, res_2 =[], []\n",
        "  for i in input:\n",
        "    if i[0] == 'spam' :\n",
        "      # Convert the message to lowercase, remove punctuation, and split it into words\n",
        "      res_1.append(('spam',(i[1].lower().translate(str.maketrans('', '', string.punctuation))).split()))\n",
        "    elif i[0] == 'ham' :\n",
        "      res_2.append(('ham',(i[1].lower().translate(str.maketrans('', '', string.punctuation))).split()))\n",
        "  res_combined = res_1 + res_2\n",
        "  return res_combined\n",
        "\n",
        "res21 = map_words_in_msg(data)\n",
        "res21[:2]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIN357SUdAIM"
      },
      "source": [
        "### <strong> 2.2. Shuffle </strong>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "TWcb7-6ac_fB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "free : [('spam', ('free', 1)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 1)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 1)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 1)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 1)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 3)), ('spam', ('free', 1)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 2)), ('spam', ('free', 2)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('spam', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 2)), ('ham', ('free', 2)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1)), ('ham', ('free', 1))]\n",
            "entry : [('spam', ('entry', 2)), ('spam', ('entry', 2)), ('spam', ('entry', 1)), ('spam', ('entry', 1)), ('spam', ('entry', 1)), ('spam', ('entry', 1)), ('spam', ('entry', 2)), ('spam', ('entry', 2)), ('spam', ('entry', 1)), ('spam', ('entry', 1)), ('spam', ('entry', 1)), ('spam', ('entry', 1)), ('spam', ('entry', 2)), ('spam', ('entry', 2)), ('spam', ('entry', 1)), ('spam', ('entry', 1)), ('spam', ('entry', 1)), ('spam', ('entry', 1)), ('spam', ('entry', 2)), ('spam', ('entry', 2)), ('spam', ('entry', 1)), ('spam', ('entry', 1)), ('spam', ('entry', 2)), ('spam', ('entry', 2)), ('spam', ('entry', 1)), ('spam', ('entry', 1))]\n"
          ]
        }
      ],
      "source": [
        "#Provide here few comment lines that describe your shuffle function\n",
        "\n",
        "def shuffle_words_in_msg(input):\n",
        "    \"\"\"\n",
        "    This function takes an input dataset consisting of labels and words. It creates a reversed dictionary\n",
        "    where words are the keys, and the values are lists of (label, word count) pairs.\n",
        "\n",
        "    :param input: Input data consisting of tuples (label, words)\n",
        "    :return: A dictionary where each word is a key, and the values are lists of (label, word count) pairs\n",
        "    \"\"\"\n",
        "    reversed_dict = defaultdict(list)\n",
        "    for label, words in input:\n",
        "        for word in words:\n",
        "            word_count = (word, words.count(word))\n",
        "            reversed_dict[word].append((label, word_count))\n",
        "    return dict(reversed_dict)\n",
        "\n",
        "res22 = shuffle_words_in_msg(res21)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BvqpZn1dFVG"
      },
      "source": [
        "### <strong> 2.3. Reduce </strong>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "W00vgZe3c_TB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The ouput interprets like this: (word, spam count, ham count)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('i', 45, 2185), ('you', 287, 1837), ('to', 686, 1554), ('the', 204, 1118)]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Provide here few comment lines that describe your reduce function\n",
        "def reduce_words_in_msg(dict):\n",
        "    \"\"\"\n",
        "    This function takes a dictionary of word occurrences and categorizes them into 'spam' and 'ham' counts for each word.\n",
        "    It then sorts the words based on 'ham' occurrences in descending order.\n",
        "\n",
        "    :param word_counts: Dictionary where keys are words, and values are lists of (label, word count) pairs\n",
        "    :return: A list of tuples where each tuple contains (word, spam count, ham count) sorted by ham count in descending order\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    for k in dict.keys():\n",
        "        counts = dict[k]\n",
        "        spam_count = len([item for item in counts if item[0] == 'spam'])\n",
        "        ham_count = len([item for item in counts if item[0] == 'ham'])\n",
        "        res.append((k, spam_count, ham_count))\n",
        "    res = sorted(res, key=lambda x: x[2], reverse=True)\n",
        "    #Sort the word occurrences in 'ham' in descending order\n",
        "    return res\n",
        "\n",
        "res32 = reduce_words_in_msg(res22)\n",
        "print('The ouput interprets like this: (word, spam count, ham count)')\n",
        "res32[:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYM8qhAuSju5",
        "scrolled": true
      },
      "source": [
        "\n",
        "## Part 3 - Average\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Describe and implement a Map-Reduce algorithm that gives, the average number of word in spam messages and the everage number of words in ham messages.\n",
        "\n",
        "You can decide if removing stop words is significant or not for this analysis.\n",
        "\n",
        "Notice that this exercise shares some similarities with one of the previous exercises. Think how and if you can modify (generalize) one of the functions already implemented before.\n",
        "\n",
        "</font>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBz5e3kudQYK"
      },
      "source": [
        "### <strong> 3.1. Map </strong>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zPywU-Hu6UF7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('spam',\n",
              "  ['free',\n",
              "   'entry',\n",
              "   '2',\n",
              "   'a',\n",
              "   'wkly',\n",
              "   'comp',\n",
              "   'win',\n",
              "   'fa',\n",
              "   'cup',\n",
              "   'final',\n",
              "   'tkts',\n",
              "   '21st',\n",
              "   'may',\n",
              "   '2005',\n",
              "   'text',\n",
              "   'fa',\n",
              "   '87121',\n",
              "   'receive',\n",
              "   'entry',\n",
              "   'questionstd',\n",
              "   'txt',\n",
              "   'ratetcs',\n",
              "   'apply',\n",
              "   '08452810075over18s']),\n",
              " ('spam',\n",
              "  ['freemsg',\n",
              "   'hey',\n",
              "   'there',\n",
              "   'darling',\n",
              "   'its',\n",
              "   'been',\n",
              "   '3',\n",
              "   'weeks',\n",
              "   'now',\n",
              "   'no',\n",
              "   'word',\n",
              "   'back',\n",
              "   'id',\n",
              "   'like',\n",
              "   'some',\n",
              "   'fun',\n",
              "   'you',\n",
              "   'up',\n",
              "   'for',\n",
              "   'still',\n",
              "   'tb',\n",
              "   'ok',\n",
              "   'xxx',\n",
              "   'std',\n",
              "   'chgs',\n",
              "   'send',\n",
              "   'å£150',\n",
              "   'rcv'])]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "stop_words = [\"the\", \"and\", \"is\", \"in\", \"it\", \"an\", \"this\", \"to\", \"of\"]\n",
        "\n",
        "def map_average(input):\n",
        "    \"\"\"\n",
        "    Maps input data to lowercase, removes punctuation, and splits text into words.\n",
        "\n",
        "    Args.\n",
        "        input (list of tuples): Input data containing labels and text.\n",
        "\n",
        "    Returns.\n",
        "        list of tuples: Each tuple contains a list of labels and words.\n",
        "    \"\"\"\n",
        "    res_1, res_2 = [], []\n",
        "    for i in input:\n",
        "        if i[0] == 'spam':\n",
        "            # Process to get a list of lowercase words without punctuation and stop words.\n",
        "            words = [word for word in i[1].lower().translate(str.maketrans('', '', string.punctuation)).split() if word not in stop_words]\n",
        "            res_1.append(('spam', words))\n",
        "        elif i[0] == 'ham':\n",
        "            words = [word for word in i[1].lower().translate(str.maketrans('', '', string.punctuation)).split() if word not in stop_words]\n",
        "            res_2.append(('ham', words))\n",
        "    res_combined = res_1 + res_2\n",
        "    return res_combined\n",
        "\n",
        "res31 = map_average(data)\n",
        "res31[:2]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO4MqLMmdPgn"
      },
      "source": [
        "### <strong> 3.2. Shuffle </strong>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dKv434SfdO1m"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('spam', 24), ('spam', 28), ('spam', 24), ('spam', 25), ('spam', 22)]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Provide here few comment lines that describe your shuffle function\n",
        "\n",
        "def shuffle_average(input):\n",
        "    \"\"\"\n",
        "    Computes the average number of words in each category\n",
        "\n",
        "    Args:\n",
        "        input (list of tuples): Input data containing labels and text data.\n",
        "\n",
        "    Returns:\n",
        "        list of tuples: Each tuple contains a category label ('spam' or 'ham') and the average number of words.\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    for i in input:\n",
        "      res.append((i[0],len(i[1])))\n",
        "    return res\n",
        "\n",
        "res32 = shuffle_average(res31)\n",
        "\n",
        "res32[:5]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5nGZalOdT5l"
      },
      "source": [
        "### <strong> 3.3. Reduce </strong>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nd3-3DTJdOpq",
        "outputId": "dafc18ab-acf1-4b13-d4a1-e848972abb26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('spam', 21.578), ('ham', 12.644)]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Provide here few comment lines that describe your reduce function\n",
        "\n",
        "def  reduce_average(input):\n",
        "    \"\"\"\n",
        "    Computes the average word count for each category (e.g., 'spam' or 'ham') based on the input data.\n",
        "\n",
        "    Args:\n",
        "        input (list of tuples): Input data containing labels and word counts.\n",
        "\n",
        "    Returns:\n",
        "        list of tuples: Each tuple contains a category label ('spam' or 'ham') and the average word count (rounded to 3 decimal places).\n",
        "    \"\"\"\n",
        "    word_counts = {}\n",
        "    for label, count in input:\n",
        "        if label not in word_counts:\n",
        "            word_counts[label] = {'total_words': 0, 'count': 0}\n",
        "        word_counts[label]['total_words'] += count\n",
        "        word_counts[label]['count'] += 1\n",
        "\n",
        "    averages = []\n",
        "    for label, counts in word_counts.items():\n",
        "        average = counts['total_words'] / counts['count']\n",
        "        averages.append((label, round(average,3)))\n",
        "\n",
        "    return averages\n",
        "\n",
        "res33 = reduce_average(res32)\n",
        "res33"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o-3JT1L659s"
      },
      "source": [
        "Look at your code. Can you use a combine operation?\n",
        "\n",
        "If you cannot directly think about how you can represent the key-value pairs in a way that allows you to take advantage of a combine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg-GIFxidgLD"
      },
      "source": [
        "### <strong> 3.4. Advanced Combine </strong>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "dEOIsmdE7LQh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('spam', 21.578), ('ham', 12.644)]\n"
          ]
        }
      ],
      "source": [
        "# distribute the data\n",
        "\n",
        "def distribute_dataset(data, n):\n",
        "  '''\n",
        "  split the data into n chunks\n",
        "\n",
        "  args:\n",
        "  data->list:data to be distributed\n",
        "  n->int:numbers of chunks divided\n",
        "\n",
        "  returns:\n",
        "  lists of list\n",
        "  '''\n",
        "  chunk_size = len(data)//n\n",
        "  chunks = [data[i:i+chunk_size] for i in range(0,len(data),chunk_size)]\n",
        "  return chunks\n",
        "\n",
        "#split the data into 4 chunks\n",
        "chunks = distribute_dataset(data, 4)\n",
        "\n",
        "\n",
        "def map_advanced_average(input):\n",
        "    \"\"\"\n",
        "    Maps input data to lowercase, removes punctuation, and splits text into words.\n",
        "    Removes stop words from the list of words.\n",
        "\n",
        "    Args:\n",
        "        input (list of tuples): Input data containing labels and text.\n",
        "\n",
        "    Returns:\n",
        "        list of tuples: Each tuple contains a label ('spam' or 'ham') and the count of non-stop words.\n",
        "    \"\"\"\n",
        "    res_1, res_2 = [], []\n",
        "    for i in input:\n",
        "        if i[0] == 'spam':\n",
        "\n",
        "            words = [word for word in i[1].lower().translate(str.maketrans('', '', string.punctuation)).split() if word not in stop_words]\n",
        "            res_1.append(('spam', len(words)))\n",
        "        elif i[0] == 'ham':\n",
        "\n",
        "            words = [word for word in i[1].lower().translate(str.maketrans('', '', string.punctuation)).split() if word not in stop_words]\n",
        "            res_2.append(('ham', len(words)))\n",
        "    res_combined = res_1 + res_2\n",
        "    return res_combined\n",
        "\n",
        "map_results = [map_advanced_average(chunk) for chunk in chunks]\n",
        "\n",
        "def shuffle_advanced_average(input):\n",
        "    \"\"\"\n",
        "    Shuffles and combines the intermediate data to accumulate the sum and count of word counts for 'spam' and 'ham' messages.\n",
        "\n",
        "    Args:\n",
        "        input (list of tuples): Intermediate data after mapping.\n",
        "\n",
        "    Returns:\n",
        "        list of tuples: Each tuple contains a label ('spam' or 'ham'), the sum of word counts, and the count of messages for that label.\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    sum_spam, sum_ham = 0, 0\n",
        "    counts_spam, counts_ham =0, 0\n",
        "\n",
        "    for label, word_count in input:\n",
        "        if label == 'spam':\n",
        "            sum_spam += word_count\n",
        "            counts_spam += 1\n",
        "        elif label == 'ham':\n",
        "            sum_ham += word_count\n",
        "            counts_ham += 1\n",
        "\n",
        "    res.append(('spam', sum_spam, counts_spam))\n",
        "    res.append(('ham', sum_ham,counts_ham))\n",
        "\n",
        "    return res\n",
        "\n",
        "combined_map_results = []\n",
        "for result in map_results:\n",
        "    combined_map_results.extend(result)\n",
        "\n",
        "def reduce_advanced_average(input):\n",
        "    \"\"\"\n",
        "    Reduces the shuffled data to calculate the average word count for 'spam' and 'ham' messages.\n",
        "    Rounds the averages to three decimal places.\n",
        "\n",
        "    Args:\n",
        "        input (list of tuples): Shuffled and combined data.\n",
        "\n",
        "    Returns:\n",
        "        list of tuples: Each tuple contains a label ('spam' or 'ham') and the calculated average word count, rounded to three decimal places.\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    average_ham, average_spam = 0, 0\n",
        "    for i in input:\n",
        "        if i[0] == 'spam':\n",
        "            average_spam = i[1] / i[2]\n",
        "        if i[0] == 'ham':\n",
        "            average_ham = i[1] / i[2]\n",
        "    res.append(('spam', round((average_spam),3)))\n",
        "    res.append(('ham', round((average_ham), 3)))\n",
        "    return res\n",
        "\n",
        "shuffled_results = shuffle_advanced_average(combined_map_results)\n",
        "final_results = reduce_advanced_average(shuffled_results)\n",
        "print(final_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6i8ZaKidqgOD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "name": "BE4-Spark.ipynb",
    "vscode": {
      "interpreter": {
        "hash": "fd75362e27048f1ead3b65beb4812b1da3d387150557ce53b099093c32022a5e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
