{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0S3PBIZQ2NH"
      },
      "source": [
        "<h1><center>Big Data Algorithms Techniques & Platforms</center></h1>\n",
        "<h2>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "<center>Spark and DataFrames</center>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g53MVOhDv7Oe"
      },
      "source": [
        "#A. Analysis of the ``Great Expectations`` - Charles Dickens\n",
        "\n",
        "Suppose you have a file containing the text of the ``Great Expectation``, a novel written in English by Charles Dickens. You can see an excerpt below :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwiHRqBNv8Tf"
      },
      "source": [
        "\"I pointed to where our village lay, on the flat in-shore among the\n",
        "alder-trees and pollards, a mile or more from the church.\n",
        "\n",
        "The man, after looking at me for a moment, turned me upside down, and\n",
        "emptied my pockets. There was nothing in them but a piece of bread.\n",
        "When the church came to itself,—for he was so sudden and strong that he\n",
        "made it go head over heels before me, and I saw the steeple under my\n",
        "feet,—when the church came to itself, I say, I was seated on a high\n",
        "tombstone, trembling while he ate the bread ravenously.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI8DIAIBdPQ7"
      },
      "source": [
        "## <strong>Exercise 0.</strong> Support functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vl2keC7qQ1B8"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar zxvf spark-3.5.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rsm4lk4mnTAQ",
        "outputId": "823e591c-ee45-4b3d-ddd3-68a5479fa432"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initialization successful!\n"
          ]
        }
      ],
      "source": [
        "# Basic Python libraries\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import random as rn\n",
        "from collections import Counter\n",
        "\n",
        "# Initializing Spark\n",
        "import findspark\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession, Window\n",
        "\n",
        "# PySpark SQL functions and types\n",
        "from pyspark.sql.functions import (\n",
        "    mean, col, min, avg, count, coalesce, lit, explode, length, split, regexp_replace,\n",
        "    trim, udf, size, round, rand\n",
        ")\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "# PySpark ML features\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "# NLTK functionalities\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\")\n",
        "sc = SparkContext(conf = conf)\n",
        "print(\"initialization successful!\")\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AverageSentenceLength\").getOrCreate()\n",
        "\n",
        "seed_value=0\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "bRoe2rJxdIvk"
      },
      "outputs": [],
      "source": [
        "### Write here all the import function and the support function you need for processing the text\n",
        "\n",
        "filepath = '/content/great_expectations.txt'\n",
        "\n",
        "# Use the StopWordsRemover tool from PySpark ML to get a list of common stopwords\n",
        "remover = StopWordsRemover()\n",
        "stopwords = set(remover.getStopWords())\n",
        "\n",
        "chapters = [\n",
        "    \"Chapter I.\", \"Chapter II.\", \"Chapter III.\", \"Chapter IV.\", \"Chapter V.\",\n",
        "    \"Chapter VI.\", \"Chapter VII.\", \"Chapter VIII.\", \"Chapter IX.\", \"Chapter X.\",\n",
        "    \"Chapter XI.\", \"Chapter XII.\", \"Chapter XIII.\", \"Chapter XIV.\", \"Chapter XV.\",\n",
        "    \"Chapter XVI.\", \"Chapter XVII.\", \"Chapter XVIII.\", \"Chapter XIX.\", \"Chapter XX.\",\n",
        "    \"Chapter XXI.\", \"Chapter XXII.\", \"Chapter XXIII.\", \"Chapter XXIV.\", \"Chapter XXV.\",\n",
        "    \"Chapter XXVI.\", \"Chapter XXVII.\", \"Chapter XXVIII.\", \"Chapter XXIX.\", \"Chapter XXX.\",\n",
        "    \"Chapter XXXI.\", \"Chapter XXXII.\", \"Chapter XXXIII.\", \"Chapter XXXIV.\", \"Chapter XXXV.\",\n",
        "    \"Chapter XXXVI.\", \"Chapter XXXVII.\", \"Chapter XXXVIII.\", \"Chapter XXXIX.\", \"Chapter XL.\",\n",
        "    \"Chapter XLI.\", \"Chapter XLII.\", \"Chapter XLIII.\", \"Chapter XLIV.\", \"Chapter XLV.\",\n",
        "    \"Chapter XLVI.\", \"Chapter XLVII.\", \"Chapter XLVIII.\", \"Chapter XLIX.\", \"Chapter L.\",\n",
        "    \"Chapter LI.\", \"Chapter LII.\", \"Chapter LIII.\", \"Chapter LIV.\", \"Chapter LV.\",\n",
        "    \"Chapter LVI.\", \"Chapter LVII.\", \"Chapter LVIII.\", \"Chapter LIX.\"           ]\n",
        "\n",
        "# This function retrieves the index positions of each chapter in the RDD.\n",
        "def get_chapter_indices(rdd, chapters):\n",
        "    # Dictionary to hold the chapter names and their corresponding index positions.\n",
        "    indices = {}\n",
        "\n",
        "    # Loop through each chapter.\n",
        "    for chapter in chapters:\n",
        "        # Get the index of the chapter in the RDD.\n",
        "        index = rdd.zipWithIndex().filter(lambda x: x[0] == chapter).map(lambda x: x[1]).collect()\n",
        "\n",
        "        # If the index exists, add it to the indices dictionary with the chapter name as the key.\n",
        "        if index:\n",
        "            indices[chapter] = index[0]\n",
        "\n",
        "    # Return the indices dictionary.\n",
        "    return indices\n",
        "\n",
        "\n",
        "# This function retrieves the content of each chapter using the indices found earlier.\n",
        "def get_chapter_content(rdd, chapter_indices):\n",
        "    # Dictionary to hold the chapter names and their corresponding content.\n",
        "    chapters_content = {}\n",
        "\n",
        "    # Convert the chapter names into a list.\n",
        "    chapter_names = list(chapter_indices.keys())\n",
        "\n",
        "    # Loop through each chapter name.\n",
        "    for i in range(len(chapter_names)):\n",
        "        start_index = chapter_indices[chapter_names[i]]\n",
        "\n",
        "        # If there's a next chapter, get its index. Otherwise, set it to None.\n",
        "        end_index = chapter_indices[chapter_names[i+1]] if i+1 < len(chapter_names) else None\n",
        "\n",
        "        # Extract content between start and end indices.\n",
        "        content = rdd.zipWithIndex().filter(lambda x: start_index < x[1] and (end_index is None or x[1] < end_index)).map(lambda x: x[0]).collect()\n",
        "\n",
        "        # Combine the content into a single string.\n",
        "        chapters_content[chapter_names[i]] = ' '.join(content)\n",
        "\n",
        "    # Return the dictionary with chapter names and their content.\n",
        "    return chapters_content\n",
        "\n",
        "# This function creates an RDD with each record as a tuple containing chapter name and a sentence from that chapter.\n",
        "def get_sentences_rdd(chapters_content, sc):\n",
        "    # List to hold the chapter-sentence pairs.\n",
        "    data = []\n",
        "\n",
        "    # Loop through each chapter's content.\n",
        "    for chapter, content in chapters_content.items():\n",
        "\n",
        "        # Split the content into sentences (assuming the function split_sentences() exists).\n",
        "        sentences = split_sentences(content)\n",
        "\n",
        "        # Append each sentence with its chapter name to the data list.\n",
        "        for sentence in sentences:\n",
        "            data.append((chapter, sentence))\n",
        "\n",
        "    # Convert the data list into an RDD and return.\n",
        "    return sc.parallelize(data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jeq1OBoKc7aS"
      },
      "source": [
        "## <strong>Exercise 1</strong> Word number in sentences\n",
        "\n",
        "The length of a sentence is the number of words that compose the sentence.\n",
        "\n",
        "Write and comment on the set of Spark operations that return how many sentences we have for each available length in the text. You must also show the five most common lengths.\n",
        "\n",
        "Notice that in the available text, a sentence is a set of lines that ends with a strong punctuation mark (i.e., \".\", \"!\", \"?\", etc.).\n",
        "\n",
        "Notice that:\n",
        "\n",
        "* The novel starts after the <code> *** START OF THE PROJECT GUTENBERG EBOOK GREAT EXPECTATIONS ***</code>\n",
        "\n",
        "* <code> [Illustration] </code> is not a sentence\n",
        "\n",
        "* <code>Chapter VIII.</code> is not a sentence\n",
        "\n",
        "You can introduce your constraints for the parsing. Multiple solutions and points of view are correct. You must comment on your point of view (i.e., the definition of ``sentence'' in your analysis).\n",
        "\n",
        "You can choose if you are considering the stop-words in the count: add your point of view in the comment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tbanmLEiuLU",
        "outputId": "5fc2e984-17da-4d9a-bea8-bb9a95d1432c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(5, 493),\n",
              " (6, 474),\n",
              " (9, 473),\n",
              " (4, 468),\n",
              " (3, 455),\n",
              " (8, 439),\n",
              " (7, 423),\n",
              " (2, 393),\n",
              " (10, 392),\n",
              " (11, 392)]"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#read File and conver it into RDD\n",
        "rdd = sc.textFile(filepath)\n",
        "\n",
        "# Find the index of the start marker\n",
        "start_idx = rdd.zipWithIndex().filter(lambda x: x[0] == '*** START OF THE PROJECT GUTENBERG EBOOK GREAT EXPECTATIONS ***').first()[1]\n",
        "\n",
        "# Filter out all content before the start marker\n",
        "filtered_rdd = rdd.zipWithIndex().filter(lambda x: x[1] > start_idx).map(lambda x: x[0])\n",
        "\n",
        "# Remove any illustration from the RDD\n",
        "filtered_rdd = filtered_rdd.filter(lambda line: line != '[Illustration]')\n",
        "\n",
        "# Filter out lines that start with 'Chapter' to remove chapter headings\n",
        "filtered_rdd = filtered_rdd.filter(lambda line: not line.startswith(' Chapter'))\n",
        "\n",
        "# Remove any empty lines or lines with only white spaces\n",
        "filtered_rdd = filtered_rdd.filter(lambda x: x.strip() != '')\n",
        "\n",
        "# Combine all the lines into a single string to form the complete text of the book\n",
        "entire_text = filtered_rdd.reduce(lambda a, b: a + \" \" + b)\n",
        "\n",
        "# Split the entire text into individual sentences using regex\n",
        "sentences = re.split('(?<=[.!?])\\s+', entire_text)\n",
        "\n",
        "# Convert the list of sentences back into an RDD\n",
        "sentences_rdd = sc.parallelize(sentences)\n",
        "\n",
        "# Using map, split each sentence into words\n",
        "words_rdd = sentences_rdd.map(lambda sentence: sentence.split())\n",
        "\n",
        "# Filter out any stopwords from the words of each sentence and convert words to lowercase\n",
        "cleaned_rdd = words_rdd.map(lambda words: [word for word in words if word.lower() not in stopwords])\n",
        "\n",
        "# Using map, return the length of each sentence\n",
        "sentence_lengths = cleaned_rdd.map(lambda sentence: (len(sentence), 1))\n",
        "\n",
        "# Reduce by key, counting how many sentences of each length\n",
        "sentence_counts = sentence_lengths.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Sort the result by ascending orders\n",
        "sorted_sentence_counts = sentence_counts.sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "#show result:Display the 10 most common sentence lengths and their counts\n",
        "sorted_sentence_counts.take(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldd9xwUNdGGF"
      },
      "source": [
        "## <strong>Exercise 2.</strong> The average length of sentences in the entire novel\n",
        "Write and comment on the set of Spark operations that returns and shows the average length of the sentences in the novel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpfbBE_KlPe1",
        "outputId": "35838115-68e4-4672-9a12-7339dc3683d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11.58799504950495"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "############## WRITE YOUR CODE HERE ##############\n",
        "\n",
        "# Compute the total length and total count of sentences\n",
        "total_length, total_count = sentence_lengths.reduce(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
        "\n",
        "# Calculate average\n",
        "average_length = total_length / total_count\n",
        "average_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9qdDteilCiO"
      },
      "source": [
        "## <strong>Exercise 3.</strong> Average length in chapters\n",
        "\n",
        "Write and comment on the set of Spark operations that returns and shows the average length of sentences in each chapter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iT5NUg6OzZg3",
        "outputId": "232f20be-7128-4d1a-eca7-2c603832a6f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+---------+\n",
            "|         Chapter|AvgLength|\n",
            "+----------------+---------+\n",
            "|  Chapter XXXIX.|    10.39|\n",
            "|    Chapter XIX.|    11.21|\n",
            "|     Chapter IV.|     9.59|\n",
            "| Chapter XXXIII.|    12.43|\n",
            "| Chapter XXVIII.|    12.35|\n",
            "|    Chapter LIX.|    14.17|\n",
            "|   Chapter LIII.|    11.59|\n",
            "|   Chapter XLVI.|    12.61|\n",
            "|    Chapter XIV.|    13.22|\n",
            "|   Chapter XXXI.|    13.16|\n",
            "|     Chapter IX.|     9.75|\n",
            "|   Chapter XIII.|    13.37|\n",
            "|    Chapter VII.|    13.48|\n",
            "|    Chapter XII.|    13.77|\n",
            "|      Chapter L.|    12.75|\n",
            "|    Chapter LVI.|    12.43|\n",
            "|  Chapter XLIII.|    10.18|\n",
            "|     Chapter LV.|     11.1|\n",
            "| Chapter XLVIII.|     9.85|\n",
            "|Chapter XXXVIII.|    12.98|\n",
            "+----------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get the indices of each chapter in the filtered RDD using the previously defined function.\n",
        "chapter_indices = get_chapter_indices(filtered_rdd, chapters)\n",
        "\n",
        "# Using the previously defined get_chapter_content function to obtain the content of each chapter.\n",
        "chapters_content = get_chapter_content(filtered_rdd, chapter_indices)\n",
        "\n",
        "# Get an RDD of chapter-sentence pairs using the content retrieved above.\n",
        "sentences_rdd = get_sentences_rdd(chapters_content, spark.sparkContext)\n",
        "\n",
        "# Convert the RDD to a DataFrame with columns \"Chapter\" and \"Sentence\".\n",
        "df = spark.createDataFrame(sentences_rdd, [\"Chapter\", \"Sentence\"])\n",
        "\n",
        "# Use the split function to break the sentences into words.\n",
        "# The result is stored in a new column \"Words\".\n",
        "df = df.withColumn(\"Words\", split(col(\"Sentence\"), \" \"))\n",
        "\n",
        "# Use the StopWordsRemover to remove stopwords\n",
        "# The cleaned words are stored in a new column \"CleanedWords\".\n",
        "remover = StopWordsRemover(inputCol=\"Words\", outputCol=\"CleanedWords\")\n",
        "df = remover.transform(df)\n",
        "\n",
        "# Calculate the length of the cleaned sentences (i.e., the number of words)\n",
        "# and store the result in a new column \"Length\".\n",
        "df = df.withColumn(\"Length\", size(col(\"CleanedWords\")))\n",
        "\n",
        "# Group the DataFrame by the \"Chapter\" column and compute the average length of cleaned sentences for each chapter.\n",
        "# The result is rounded to 2 decimal places and stored in a new column \"AvgLength\".\n",
        "result = df.groupBy(\"Chapter\").agg(round(avg(\"Length\"), 2).alias(\"AvgLength\"))\n",
        "\n",
        "# Display the results.\n",
        "result.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgxgDiJlKn8s"
      },
      "source": [
        "## <strong>Exercise 4.</strong> The most repeated words in the sentences\n",
        "\n",
        "Write and comment on the set of Spark operations that returns the ten most repeated words in a sentence and their repetition rate.\n",
        "\n",
        "For example, the most repeated word in this set of sentences is <code> cat </code>, with an average repetition rate of 2. The word <code> table </code> is not considered as repeated.\n",
        "\n",
        "* The cat is on the cat table.\n",
        "* The table is red.\n",
        "* The wooden table is broken.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCNmkkXHKmNU",
        "outputId": "cd9c99a0-3c27-458c-e204-e7f241313378"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('bit', 6), ('may', 5), ('“And', 5), ('rul', 5), ('looked', 5), ('go', 5), ('going', 5), ('time', 4), ('now,', 4), ('him,', 4)]\n"
          ]
        }
      ],
      "source": [
        "#  Counting words in each sentence\n",
        "sentences_rdd_reuse = cleaned_rdd.map(lambda words: Counter(words))\n",
        "\n",
        "# Filtering out repetitive words from each sentence\n",
        "repeated_words_rdd = sentences_rdd_reuse.flatMap(lambda counts: [(word, count) for word, count in counts.items() if count > 1])\n",
        "\n",
        "# 3. Select the 10 words with the highest repetition rate from all sentences\n",
        "top_10_repeated_words = repeated_words_rdd.takeOrdered(10, key=lambda x: -x[1])\n",
        "\n",
        "print(top_10_repeated_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw2bvV93KmAW"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a0nrGmRQ7Yk"
      },
      "source": [
        "# B. Coffee Dataset\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "For running this series of exercises, we are going to use a dataset coming from <a href=\"https://www.kaggle.com/datasets/schmoyote/coffee-reviews-dataset\">Kaggle</a>.\n",
        "\n",
        "As stated in the description of the dataset:\n",
        "\"Dataset contains information about coffee production and consumption.\n",
        "\n",
        "All data are available from the official ICO website:\n",
        "https://www.ico.org/new_historical.asp\".\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "### The dataset\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "The dataset is in a .csv file, and among the columns, you can find:\n",
        "\n",
        "<ul>\n",
        "<li> <code>name</code> The name of the blend </li>\n",
        "<li><code>roaster</code> The name of the roaster</li>\n",
        "<li><code>roast</code> The type of roast</li>\n",
        "<li><code>loc_country</code> The country of the roaster</li>\n",
        "<li><code>...</code> ...</li>\n",
        "</ul>\n",
        "</p>\n",
        "</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFhk7ZoBvE0C",
        "outputId": "767f5e64-1ba2-4318-ec57-7aac5c5ddfea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "great_expectations.txt\tsample_data  spark-3.5.0-bin-hadoop3  spark-3.5.0-bin-hadoop3.tgz\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-FdlIK_9tT3",
        "outputId": "6e28e4ed-c3c8-429c-f92f-a9fc61d73a8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CThDJ818ANV",
        "outputId": "23872425-5f6e-4100-ca85-2065c1f2336c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading coffee-reviews-dataset.zip to /content\n",
            "\r  0% 0.00/569k [00:00<?, ?B/s]\n",
            "\r100% 569k/569k [00:00<00:00, 45.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "! kaggle datasets download schmoyote/coffee-reviews-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ-P2NQd9uKI",
        "outputId": "95748f65-a9af-4331-d2a4-15eed686941e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  coffee-reviews-dataset.zip\n",
            "  inflating: coffee_analysis.csv     \n",
            "  inflating: simplified_coffee.csv   \n"
          ]
        }
      ],
      "source": [
        "!unzip coffee-reviews-dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr5auOga9X0Q"
      },
      "source": [
        "## <strong> Spark and Pandas.</strong>\n",
        "\n",
        "For this set of exercises, you must import data in Spark. After this first import, you can pass any dataset to Pandas for data analysis. At the end of each exercise (when the question is pertinent), you must return (reconvert) the dataframe in Spark.\n",
        "\n",
        "Each time you do this conversion you must comment about this. Example:\n",
        "\n",
        "<code> # creating a Spark dataframe </code>\n",
        "\n",
        "<code> df = ... </code>\n",
        "\n",
        "<code> # using Pandas and creating a Pandas dataframe </code>\n",
        "\n",
        "<code> dfp = df. ... </code>\n",
        "\n",
        "<code> # back to Spark </code>\n",
        "\n",
        "<code> dfs = ... </code>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "GDob14Yt8MLQ"
      },
      "outputs": [],
      "source": [
        "# Add your imports\n",
        "\n",
        "# Create a Spark session\n",
        "coffee_analysis = '/content/coffee_analysis.csv'\n",
        "simplified_coffee = '/content/simplified_coffee.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBNt1hUkTGUQ"
      },
      "source": [
        "## <strong> Exercise 5.</strong> First import and data type\n",
        "Import the .csv file in Spark DataFrame and show the structure of the dataframe.\n",
        "\n",
        "Check and comment about the data type of each column. As you know, a good data analysis always starts from understanding your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "bM5Oi6vMR5OC",
        "outputId": "427c2b8c-e563-4ae5-a253-9d1554363c8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- roaster: string (nullable = true)\n",
            " |-- roast: string (nullable = true)\n",
            " |-- loc_country: string (nullable = true)\n",
            " |-- origin_1: string (nullable = true)\n",
            " |-- origin_2: string (nullable = true)\n",
            " |-- 100g_USD: double (nullable = true)\n",
            " |-- rating: integer (nullable = true)\n",
            " |-- review_date: string (nullable = true)\n",
            " |-- desc_1: string (nullable = true)\n",
            " |-- desc_2: string (nullable = true)\n",
            " |-- desc_3: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nname: string\\nthis column will typically contain textual descriptions.\\n\\nroaster: string\\nSpecifies the roaster that roasted the coffee beans.\\n\\nroast: string\\nDescribes the level of roast for the coffee, e.g., light, medium, dark.\\nThis is categorical data, and being stored as a string means it can accept various descriptors.\\n\\nloc_country: string\\nThe country where the coffee beans were sourced or where they originated.\\nStored as a string, this column can take on the names of countries or regions.\\n\\norigin_1 and origin_2: string\\nThese columns seem to provide more specific details about the coffee's origin, possibly regions, estates, or farms within the broader loc_country.\\n\\n100g_USD: double\\nRepresents the price per 100 grams of the coffee in USD.\\nThe double datatype means it's a floating-point number, allowing it to capture prices with decimal precision.\\n\\nrating: integer\\nProvides a numerical rating for the coffee, presumably on a fixed scale (e.g., 1-10).\\nreview_date: string\\nIndicates when the coffee was reviewed.\\n\\ndesc_1, desc_2, and desc_3: string\\nThese columns provide tasting notes or descriptions of the coffee's flavor profile.\\nStoring these as strings allows for a wide range of textual descriptors.\\n\\nIn summary, this dataset seems to provide a comprehensive view of different coffee types,\\ntheir origins, cost, ratings, and taste profiles. To get the most out of this data,\\nensure data quality (e.g., handle missing values, check for inconsistencies) and\\nconsider converting the review_date to a date format if you plan on performing temporal analyses.\\n\""
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Write the command that creates (reads) a Spark DataFrame and stores the reference in the dfs variable\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"CoffeeAnalysis\").getOrCreate()\n",
        "\n",
        "#'''############## WRITE HERE YOUR CODE ##############'''\n",
        "dfs = spark.read.csv(coffee_analysis,header=True,inferSchema=True)\n",
        "\n",
        "# show the DataFrame schema\n",
        "dfs.printSchema()\n",
        "\n",
        "###### Write here your comment ##############\n",
        "'''\n",
        "name: string\n",
        "this column will typically contain textual descriptions.\n",
        "\n",
        "roaster: string\n",
        "Specifies the roaster that roasted the coffee beans.\n",
        "\n",
        "roast: string\n",
        "Describes the level of roast for the coffee, e.g., light, medium, dark.\n",
        "This is categorical data, and being stored as a string means it can accept various descriptors.\n",
        "\n",
        "loc_country: string\n",
        "The country where the coffee beans were sourced or where they originated.\n",
        "Stored as a string, this column can take on the names of countries or regions.\n",
        "\n",
        "origin_1 and origin_2: string\n",
        "These columns seem to provide more specific details about the coffee's origin, possibly regions, estates, or farms within the broader loc_country.\n",
        "\n",
        "100g_USD: double\n",
        "Represents the price per 100 grams of the coffee in USD.\n",
        "The double datatype means it's a floating-point number, allowing it to capture prices with decimal precision.\n",
        "\n",
        "rating: integer\n",
        "Provides a numerical rating for the coffee, presumably on a fixed scale (e.g., 1-10).\n",
        "review_date: string\n",
        "Indicates when the coffee was reviewed.\n",
        "\n",
        "desc_1, desc_2, and desc_3: string\n",
        "These columns provide tasting notes or descriptions of the coffee's flavor profile.\n",
        "Storing these as strings allows for a wide range of textual descriptors.\n",
        "\n",
        "In summary, this dataset seems to provide a comprehensive view of different coffee types,\n",
        "their origins, cost, ratings, and taste profiles. To get the most out of this data,\n",
        "ensure data quality (e.g., handle missing values, check for inconsistencies) and\n",
        "consider converting the review_date to a date format if you plan on performing temporal analyses.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ1400g4h7fK"
      },
      "source": [
        "## <strong> Exercise 6.</strong> Data modeling choices\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Comment on the data-modeling choices and if you consider them correct from a general data-modeling point of view.\n",
        "</font>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAN0pvW65BHB",
        "outputId": "4bbd3976-3d07-4b8b-feff-a2447ee357ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+------------+-------------+--------------------+--------------------+--------+------+-------------+--------------------+--------------------+--------------------+\n",
            "|                name|             roaster|       roast|  loc_country|            origin_1|            origin_2|100g_USD|rating|  review_date|              desc_1|              desc_2|              desc_3|\n",
            "+--------------------+--------------------+------------+-------------+--------------------+--------------------+--------+------+-------------+--------------------+--------------------+--------------------+\n",
            "|“Sweety” Espresso...|              A.R.C.|Medium-Light|    Hong Kong|              Panama|            Ethiopia|   14.32|    95|November 2017|Evaluated as espr...|An espresso blend...|A radiant espress...|\n",
            "|Flora Blend Espresso|              A.R.C.|Medium-Light|    Hong Kong|              Africa|        Asia Pacific|    9.05|    94|November 2017|Evaluated as espr...|An espresso blend...|A floral-driven s...|\n",
            "|Ethiopia Shakiso ...|        Revel Coffee|Medium-Light|United States|           Guji Zone|   Southern Ethiopia|     4.7|    92|November 2017|Crisply sweet, co...|This coffee tied ...|A gently spice-to...|\n",
            "|  Ethiopia Suke Quto|         Roast House|Medium-Light|United States|           Guji Zone|       Oromia Region|    4.19|    92|November 2017|Delicate, sweetly...|This coffee tied ...|Lavender-like flo...|\n",
            "|Ethiopia Gedeb Ha...|Big Creek Coffee ...|      Medium|United States|      Gedeb District|          Gedeo Zone|    4.85|    94|November 2017|Deeply sweet, sub...|Southern Ethiopia...|A deeply and gene...|\n",
            "|Ethiopia Kayon Mo...|Red Rooster Coffe...|       Light|United States|Odo Shakiso District|           Guji Zone|    5.14|    93|November 2017|Delicate, richly ...|This coffee tied ...|A lively and cris...|\n",
            "|Ethiopia Gelgelu ...|Willoughby's Coff...|Medium-Light|United States|Yirgacheffe Growi...|   Southern Ethiopia|    3.97|    93|November 2017|High-toned, flora...|This coffee tied ...|A deeply sweet na...|\n",
            "|Ethiopia Hambela ...|Black Oak Coffee ...|Medium-Light|United States|Hambela Wamena Di...|           Guji Zone|    5.14|    93|November 2017|Very delicate, sw...|This coffee tied ...|A delicate, richl...|\n",
            "|Organic Ethiopia ...|  Wonderstate Coffee|Medium-Light|United States|    Hambela District|           Guji Zone|    5.29|    93|November 2017|High-toned, crisp...|This coffee tied ...|An inviting wet-p...|\n",
            "|     Ethiopia Sidama|Reunion Island Co...|      Medium|       Canada|Sidama (Also Sida...|   Southern Ethiopia|    3.76|    94|November 2017|Balanced, sweet-s...|This exceptional ...|An elegantly expr...|\n",
            "|    Thiriku AA Kenya|PT's Coffee Roasting|      Medium|United States|       Central Kenya|Sidama (Also Sida...|     7.2|    95|November 2017|Intense; deep, sp...|Produced by the T...|Mysterious and ex...|\n",
            "|Ethiopia Natural ...|       Kakalove Cafe|Medium-Light|       Taiwan|        Tore Village|Yirgacheffe Growi...|    3.89|    93|November 2017|Deeply crisp, lus...|\"Southern Ethiopi...|An intense cup, z...|\n",
            "|Direct Fair Trade...|Level Ground Trading|Medium-Light|       Canada|              Africa|       South America|    2.82|    92|November 2017|Evaluated as espr...|Coffees in this b...|A solid espresso ...|\n",
            "|Decaf Ethiopia Si...|        Old Soul Co.|      Medium|United States|Sidama (Also Sida...|   Southern Ethiopia|    5.73|    90|November 2017|Surprising and me...|Southern Ethiopia...|This very light-r...|\n",
            "|Ethiopia Yirgache...|              A.R.C.|Medium-Light|    Hong Kong|Yirgacheffe Growi...|   Southern Ethiopia|    6.87|    92|November 2017|Delicate, gently ...|Produced in Yirga...|A cup with a fine...|\n",
            "|Ethiopia Yirgache...|              A.R.C.|Medium-Light|    Hong Kong|Yirgacheffe Growi...|   Southern Ethiopia|    6.87|    91|November 2017|Rich, sweet-toned...|Yirgacheffe is a ...|A crisply bitters...|\n",
            "|“Fruity” Espresso...|              A.R.C.|Medium-Light|    Hong Kong|            Ethiopia|               Kenya|   11.45|    95|November 2017|Evaluated as espr...|An espresso blend...|Rich, sweet, eleg...|\n",
            "|Decaf Colombia Se...|Bootstrap Coffee ...|Medium-Light|United States|            Colombia|            Ethiopia|    4.41|    87|November 2017|Sweet, delicate, ...|Produced from tre...|Those who value a...|\n",
            "|     Ethiopia Awassa|   Paradise Roasters|Medium-Light|United States|         Yirgacheffe|            Ethiopia|    5.28|    93|November 2017|Crisply sweet, mu...|Southern Ethiopia...|An engaging, conf...|\n",
            "|     100% Kona SL-28|Hula Daddy Kona C...|Medium-Light|      Hawai'i|            Holualoa|North Kona Growin...|   33.05|    97|November 2017|Spectacularly swe...|This exceptional ...|Coffee simply doe...|\n",
            "+--------------------+--------------------+------------+-------------+--------------------+--------------------+--------+------+-------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I80ZGgPGtUd6"
      },
      "outputs": [],
      "source": [
        "###### Write here your comment\n",
        "'''\n",
        "From a general data-modeling perspective, the choices made for this dataset are largely appropriate, with a few considerations:\n",
        "\n",
        "1. **Textual Data (string)**:\n",
        "   - Columns like `name`, `roaster`, `roast`, `loc_country`, `origin_1`, `origin_2`, `desc_1`, `desc_2`, and `desc_3` are rightly modeled as strings. They represent names, descriptions, and categorical data that don't require numerical operations.\n",
        "\n",
        "2. **Numeric Data**:\n",
        "   - The choice of `double` for `100g_USD` is appropriate since prices can often have decimal values. This allows capturing precise pricing information.\n",
        "   - Using an `integer` for `rating` suggests that the rating system is based on whole numbers, which can simplify the rating process and make it more interpretable for users. However, the range and scale of this rating system should be clearly defined somewhere to provide context.\n",
        "\n",
        "3. **Date Data**:\n",
        "   - The `review_date` column is modeled as a string. While this is not inherently wrong, if the dataset will be used for time series analysis, date comparisons, or if there's a need to extract month, year, or day from the dates, it would be more beneficial to store this column as a date or timestamp data type.\n",
        "\n",
        "4. **Redundancy**:\n",
        "   - The presence of both `origin_1` and `origin_2` might imply a hierarchical relationship (e.g., country and region). If that's the case, their naming could be more descriptive to make this relationship clear. If not, the potential for redundancy should be examined.\n",
        "\n",
        "5. **Data Normalization**:\n",
        "   - If the dataset is large and contains many repeating values (like roasters or countries), considering a normalized form might be beneficial. This involves creating separate tables for entities like roasters or countries and linking them using foreign keys. This reduces redundancy and can make updates more efficient.\n",
        "\n",
        "6. **Descriptive Columns**:\n",
        "   - Having three description columns (`desc_1`, `desc_2`, and `desc_3`) might indicate multiple tasting notes for each coffee. While this structure can accommodate varied descriptors, it may limit flexibility if more tasting notes are needed in the future. An alternative approach could involve a separate table for tasting notes, linked by a coffee ID.\n",
        "\n",
        "In conclusion, the current data modeling choices are suitable for a straightforward analysis and representation of coffee data. However, as with any dataset, the modeling decisions should be aligned with the specific analytical needs and the nature of the operations intended to be performed on the data.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET9gY8MH21Ew"
      },
      "source": [
        "## <strong> Exercise 7.</strong>  Best rated coffees\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "We want to find the 5 best rated coffees.\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdVft7Fq3pPe",
        "outputId": "4667c0bb-f627-4cf3-fc98-7e8819585015"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------+\n",
            "|                name|rating|\n",
            "+--------------------+------+\n",
            "|GW01 Finca Sophia...|    98|\n",
            "|Finca Sophia Gesh...|    98|\n",
            "|     100% Kona SL-28|    97|\n",
            "|Panama Ninety Plu...|    97|\n",
            "|Kenya Karindundu ...|    97|\n",
            "|     Rukera Espresso|    97|\n",
            "|        Rukera Kenya|    97|\n",
            "|Ethiopia Natural ...|    97|\n",
            "|Ardent Ethiopia N...|    97|\n",
            "|Testi Ayla Double...|    97|\n",
            "|   Mama Cata Mokkita|    97|\n",
            "|Colombia Finca El...|    97|\n",
            "|Yemen Haraaz Red ...|    97|\n",
            "+--------------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "############## WRITE YOUR CODE HERE ##############\n",
        "# Sort by rating in descending order and take the first 5 rows\n",
        "fifth_rating = dfs.orderBy(F.desc('rating')).limit(5).select('rating').collect()[-1][0]\n",
        "\n",
        "# All coffees with ratings equal to or higher than the fifth place were then filtered out\n",
        "result = dfs.filter(dfs.rating >= fifth_rating).select(\"name\", \"rating\").orderBy(F.desc('rating')).show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e-mami838aZ"
      },
      "source": [
        "## <strong> Exercise 8.</strong> The best 10 roasters\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "We want to find the ten best roasters. It is up to you to define and refine the ``best'' metric (best in the platform from the beginning of data collection, best in the last three years, best and with a minimum number of ratings, etc.).\n",
        "\n",
        "Add the definition of your best metric in the comments.\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIVRg-Xh6LKx",
        "outputId": "c3884182-4648-49f7-8ecc-d486c66e2716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+------------+-------------+--------------------+--------------------+--------+------+-------------+--------------------+--------------------+--------------------+\n",
            "|                name|             roaster|       roast|  loc_country|            origin_1|            origin_2|100g_USD|rating|  review_date|              desc_1|              desc_2|              desc_3|\n",
            "+--------------------+--------------------+------------+-------------+--------------------+--------------------+--------+------+-------------+--------------------+--------------------+--------------------+\n",
            "|“Sweety” Espresso...|              A.R.C.|Medium-Light|    Hong Kong|              Panama|            Ethiopia|   14.32|    95|November 2017|Evaluated as espr...|An espresso blend...|A radiant espress...|\n",
            "|Flora Blend Espresso|              A.R.C.|Medium-Light|    Hong Kong|              Africa|        Asia Pacific|    9.05|    94|November 2017|Evaluated as espr...|An espresso blend...|A floral-driven s...|\n",
            "|Ethiopia Shakiso ...|        Revel Coffee|Medium-Light|United States|           Guji Zone|   Southern Ethiopia|     4.7|    92|November 2017|Crisply sweet, co...|This coffee tied ...|A gently spice-to...|\n",
            "|  Ethiopia Suke Quto|         Roast House|Medium-Light|United States|           Guji Zone|       Oromia Region|    4.19|    92|November 2017|Delicate, sweetly...|This coffee tied ...|Lavender-like flo...|\n",
            "|Ethiopia Gedeb Ha...|Big Creek Coffee ...|      Medium|United States|      Gedeb District|          Gedeo Zone|    4.85|    94|November 2017|Deeply sweet, sub...|Southern Ethiopia...|A deeply and gene...|\n",
            "|Ethiopia Kayon Mo...|Red Rooster Coffe...|       Light|United States|Odo Shakiso District|           Guji Zone|    5.14|    93|November 2017|Delicate, richly ...|This coffee tied ...|A lively and cris...|\n",
            "|Ethiopia Gelgelu ...|Willoughby's Coff...|Medium-Light|United States|Yirgacheffe Growi...|   Southern Ethiopia|    3.97|    93|November 2017|High-toned, flora...|This coffee tied ...|A deeply sweet na...|\n",
            "|Ethiopia Hambela ...|Black Oak Coffee ...|Medium-Light|United States|Hambela Wamena Di...|           Guji Zone|    5.14|    93|November 2017|Very delicate, sw...|This coffee tied ...|A delicate, richl...|\n",
            "|Organic Ethiopia ...|  Wonderstate Coffee|Medium-Light|United States|    Hambela District|           Guji Zone|    5.29|    93|November 2017|High-toned, crisp...|This coffee tied ...|An inviting wet-p...|\n",
            "|     Ethiopia Sidama|Reunion Island Co...|      Medium|       Canada|Sidama (Also Sida...|   Southern Ethiopia|    3.76|    94|November 2017|Balanced, sweet-s...|This exceptional ...|An elegantly expr...|\n",
            "|    Thiriku AA Kenya|PT's Coffee Roasting|      Medium|United States|       Central Kenya|Sidama (Also Sida...|     7.2|    95|November 2017|Intense; deep, sp...|Produced by the T...|Mysterious and ex...|\n",
            "|Ethiopia Natural ...|       Kakalove Cafe|Medium-Light|       Taiwan|        Tore Village|Yirgacheffe Growi...|    3.89|    93|November 2017|Deeply crisp, lus...|\"Southern Ethiopi...|An intense cup, z...|\n",
            "|Direct Fair Trade...|Level Ground Trading|Medium-Light|       Canada|              Africa|       South America|    2.82|    92|November 2017|Evaluated as espr...|Coffees in this b...|A solid espresso ...|\n",
            "|Decaf Ethiopia Si...|        Old Soul Co.|      Medium|United States|Sidama (Also Sida...|   Southern Ethiopia|    5.73|    90|November 2017|Surprising and me...|Southern Ethiopia...|This very light-r...|\n",
            "|Ethiopia Yirgache...|              A.R.C.|Medium-Light|    Hong Kong|Yirgacheffe Growi...|   Southern Ethiopia|    6.87|    92|November 2017|Delicate, gently ...|Produced in Yirga...|A cup with a fine...|\n",
            "|Ethiopia Yirgache...|              A.R.C.|Medium-Light|    Hong Kong|Yirgacheffe Growi...|   Southern Ethiopia|    6.87|    91|November 2017|Rich, sweet-toned...|Yirgacheffe is a ...|A crisply bitters...|\n",
            "|“Fruity” Espresso...|              A.R.C.|Medium-Light|    Hong Kong|            Ethiopia|               Kenya|   11.45|    95|November 2017|Evaluated as espr...|An espresso blend...|Rich, sweet, eleg...|\n",
            "|Decaf Colombia Se...|Bootstrap Coffee ...|Medium-Light|United States|            Colombia|            Ethiopia|    4.41|    87|November 2017|Sweet, delicate, ...|Produced from tre...|Those who value a...|\n",
            "|     Ethiopia Awassa|   Paradise Roasters|Medium-Light|United States|         Yirgacheffe|            Ethiopia|    5.28|    93|November 2017|Crisply sweet, mu...|Southern Ethiopia...|An engaging, conf...|\n",
            "|     100% Kona SL-28|Hula Daddy Kona C...|Medium-Light|      Hawai'i|            Holualoa|North Kona Growin...|   33.05|    97|November 2017|Spectacularly swe...|This exceptional ...|Coffee simply doe...|\n",
            "+--------------------+--------------------+------------+-------------+--------------------+--------------------+--------+------+-------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-------+--------------------+-------------------+------------+--------------------+----------+--------------------+------------------+------------------+--------------+--------------------+--------------------+--------------------+\n",
            "|summary|                name|            roaster|       roast|         loc_country|  origin_1|            origin_2|          100g_USD|            rating|   review_date|              desc_1|              desc_2|              desc_3|\n",
            "+-------+--------------------+-------------------+------------+--------------------+----------+--------------------+------------------+------------------+--------------+--------------------+--------------------+--------------------+\n",
            "|  count|                2095|               2095|        2080|                2095|      2095|                2095|              2095|              2095|          2095|                2095|                2095|                2093|\n",
            "|   mean|                NULL|               NULL|        NULL|                NULL|      NULL|                NULL| 9.323312649164675| 93.11408114558472|          NULL|                NULL|                NULL|                NULL|\n",
            "| stddev|                NULL|               NULL|        NULL|                NULL|      NULL|                NULL|11.430659123983126|1.5630240639698532|          NULL|                NULL|                NULL|                NULL|\n",
            "|    min|         #She_Builds|1951 Coffee Company|        Dark|           Australia|Acatenango|\"\"\"Big Island\"\" O...|              0.12|                84|    April 2018| Bright, richly h...| Burundi is a sma...| A friendly, gent...|\n",
            "|    max|“Sweety” Espresso...|      modcup coffee|Medium-Light|United States And...|    Zambia|“Big Island” Of H...|            132.28|                98|September 2022| Rich-toned, deep...| Yirgacheffe coff...| A nuanced Kona P...|\n",
            "+-------+--------------------+-------------------+------------+--------------------+----------+--------------------+------------------+------------------+--------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfs.show()\n",
        "dfs.describe().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68a38QpWvbqZ",
        "outputId": "e3041b70-7335-4ebf-e73c-73f000231b2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|             roaster|Final_Average_Rating|\n",
            "+--------------------+--------------------+\n",
            "|Hula Daddy Kona C...|               95.13|\n",
            "|           GK Coffee|               94.25|\n",
            "|Dragonfly Coffee ...|               94.19|\n",
            "|       Kakalove Cafe|               94.16|\n",
            "|Bird Rock Coffee ...|               94.15|\n",
            "|Red Rooster Coffe...|               93.95|\n",
            "| JBC Coffee Roasters|                93.6|\n",
            "|   Paradise Roasters|               93.54|\n",
            "|Barrington Coffee...|               93.45|\n",
            "|Big Shoulders Coffee|                93.2|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "MIN_NUM_RATINGS = 20\n",
        "\n",
        "# 1. Filter out rows where roast rating is under 88\n",
        "df_filtered = dfs.filter(col('rating') >= 88)\n",
        "\n",
        "# 2. Filter out roasters with fewer than MIN_RATINGS\n",
        "windowSpec = Window.partitionBy(\"roaster\")\n",
        "\n",
        "# Calculate the number of ratings for each roaster\n",
        "df_with_num_ratings = df_filtered.withColumn(\"num_ratings\", count(\"rating\").over(windowSpec))\n",
        "\n",
        "# Filter roasters with minimum number of ratings\n",
        "df_filtered = df_with_num_ratings.filter(col(\"num_ratings\") >= MIN_NUM_RATINGS)\n",
        "\n",
        "# Use the previously defined windowSpec to calculate the average rating for each roaster\n",
        "df_with_avg_score = df_filtered.withColumn(\"Average_Rating\", avg(\"rating\").over(windowSpec))\n",
        "\n",
        "# Group by each roaster and calculate the average rating\n",
        "grouped_by_roaster = df_with_avg_score.groupBy(\"roaster\").agg(avg(\"Average_Rating\").alias(\"Final_Average_Rating\"))\n",
        "\n",
        "# Sort the roasters by the average rating and select the top 10\n",
        "top_10_roasters = grouped_by_roaster.sort(col(\"Final_Average_Rating\").desc()).limit(10)\n",
        "\n",
        "# Display the top 10 roasters and their average ratings, rounded to 2 decimal places\n",
        "top_10_roasters.select(\"roaster\", round(col(\"Final_Average_Rating\"), 2).alias(\"Final_Average_Rating\")).show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWyhAOgxvdp2"
      },
      "source": [
        "## <strong> Exercise 9.</strong> Best country\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "If you were a roaster, in which three countries would you try to set your business?\n",
        "\n",
        "Show them and refine your metric definition if necessary.\n",
        "</font>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcdMj2NlwOZl",
        "outputId": "b85f153f-f86e-43d4-c739-1b88a334a914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+-----------------+\n",
            "|         country|   weighted_score|\n",
            "+----------------+-----------------+\n",
            "|Northern Sumatra|65.28774999999999|\n",
            "|        Saraguro|          64.7695|\n",
            "|         Kochere|       64.6776875|\n",
            "+----------------+-----------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "############## WRITE YOUR CODE HERE AND THE DESCRIPTION OF YOUR \"SOMEHOW\" ##############\n",
        "\n",
        "# Country with High Rated Coffee Beans\n",
        "avg_rating_by_origin1 = dfs.groupBy(\"origin_1\").agg(mean(\"rating\").alias(\"avg_rating_origin1\"))\n",
        "avg_rating_by_origin2 = dfs.groupBy(\"origin_2\").agg(mean(\"rating\").alias(\"avg_rating_origin2\"))\n",
        "\n",
        "# Price of Coffee Beans\n",
        "avg_price_by_origin1 = dfs.groupBy(\"origin_1\").agg(mean(\"100g_USD\").alias(\"avg_price_origin1\"))\n",
        "avg_price_by_origin2 = dfs.groupBy(\"origin_2\").agg(mean(\"100g_USD\").alias(\"avg_price_origin2\"))\n",
        "\n",
        "# Join the two dataframes on their origin columns\n",
        "joined_df = avg_rating_by_origin1.alias('a') \\\n",
        "    .join(avg_rating_by_origin2.alias('b'), col('a.origin_1') == col('b.origin_2'), \"outer\") \\\n",
        "    .join(avg_price_by_origin1.alias('c'), col('a.origin_1') == col('c.origin_1'), \"outer\") \\\n",
        "    .join(avg_price_by_origin2.alias('d'), col('a.origin_1') == col('d.origin_2'), \"outer\") \\\n",
        "    .select(\n",
        "        coalesce(col(\"a.origin_1\"), col(\"b.origin_2\")).alias(\"country\"),\n",
        "        coalesce(col(\"avg_rating_origin1\"), lit(0)).alias(\"avg_rating_origin1\"),\n",
        "        coalesce(col(\"avg_rating_origin2\"), lit(0)).alias(\"avg_rating_origin2\"),\n",
        "        coalesce(col(\"avg_price_origin1\"), lit(0)).alias(\"avg_price_origin1\"),\n",
        "        coalesce(col(\"avg_price_origin2\"), lit(0)).alias(\"avg_price_origin2\")\n",
        "    )\n",
        "\n",
        "# Compute a weighted score based on average ratings and prices\n",
        "joined_df = joined_df.withColumn(\n",
        "    \"weighted_score\",\n",
        "    (col(\"avg_rating_origin1\") + col(\"avg_rating_origin2\")) / 2 * 0.7 - (col(\"avg_price_origin1\") + col(\"avg_price_origin2\")) / 2 * 0.3\n",
        ")\n",
        "\n",
        "# Sort by weighted_score in descending order and show the top countries\n",
        "top_countries = joined_df.sort(desc(\"weighted_score\")).select(\"country\", \"weighted_score\")\n",
        "\n",
        "top_countries.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IagcQAfBAC-7"
      },
      "source": [
        "## <strong> Exercise 10.</strong> Less common origin\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Show the ten less common origins of the coffees.\n",
        "\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmnpTOAvAD1N",
        "outputId": "f94cd750-95fb-4693-8a62-48f3241bcd87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------+-------+-----------+\n",
            "|              origin|count_1|count_2|total_count|\n",
            "+--------------------+-------+-------+-----------+\n",
            "|\"\"\"Big Island\"\" O...|      0|      1|          1|\n",
            "|   Acatenango Valley|      1|      0|          1|\n",
            "| Aceh Growing Region|      1|      0|          1|\n",
            "|         Aceh Tengah|      0|      1|          1|\n",
            "|      Agaro District|      1|      0|          1|\n",
            "|           Al-Haimah|      1|      0|          1|\n",
            "|           Al-Durrar|      1|      0|          1|\n",
            "|Ahuachapán Depart...|      0|      1|          1|\n",
            "|     Al Hayma Region|      1|      0|          1|\n",
            "|            Al Mahjr|      1|      0|          1|\n",
            "+--------------------+-------+-------+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "############## WRITE YOUR CODE HERE AND THE DESCRIPTION OF YOUR \"SOMEHOW\" ##############\n",
        "\n",
        "# Count occurrences for origin_1\n",
        "origin1_counts = dfs.groupBy(\"origin_1\").count().withColumnRenamed(\"origin_1\", \"origin\").withColumnRenamed(\"count\", \"count_1\")\n",
        "\n",
        "# Count occurrences for origin_2\n",
        "origin2_counts = dfs.groupBy(\"origin_2\").count().withColumnRenamed(\"origin_2\", \"origin\").withColumnRenamed(\"count\", \"count_2\")\n",
        "\n",
        "# Full outer join on the origin column\n",
        "all_origins = origin1_counts.join(origin2_counts, on=\"origin\", how=\"outer\")\n",
        "\n",
        "# Replace null values with 0\n",
        "all_origins = all_origins.withColumn(\"count_1\", coalesce(col(\"count_1\"), lit(0)))\n",
        "all_origins = all_origins.withColumn(\"count_2\", coalesce(col(\"count_2\"), lit(0)))\n",
        "\n",
        "# Compute the total count\n",
        "combined_counts = all_origins.withColumn(\"total_count\", col(\"count_1\") + col(\"count_2\"))\n",
        "\n",
        "# Sort by total_count in ascending order and show the ten least common origins\n",
        "least_common_origins = combined_counts.sort(col(\"total_count\").asc()).limit(50)\n",
        "\n",
        "least_common_origins.show(10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wpmj23nwTDk"
      },
      "source": [
        "## <strong> Exercise 11.</strong>  Propose your own analysis\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Propose here an analysis on the dataframe.\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb-wdPJEwcLF",
        "outputId": "4283a8f6-a97d-43f8-f6a3-4488e60e520a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------------------+\n",
            "|                name|   value_for_money|\n",
            "+--------------------+------------------+\n",
            "|           Cold Brew|             750.0|\n",
            "|Karen J Kona Red ...| 552.9411764705882|\n",
            "|     100% Guatemalan| 87.87878787878788|\n",
            "|      Espresso Blend| 81.41592920353983|\n",
            "|Asfaw Maru Ethiop...| 70.45454545454545|\n",
            "|Ethiopia Nano Cha...| 70.14925373134328|\n",
            "|Taiwan Natural Al...| 69.85294117647058|\n",
            "|      414 Kenya SL34| 65.24822695035462|\n",
            "|              5a Sur|61.904761904761905|\n",
            "|         5a Poniente|61.904761904761905|\n",
            "+--------------------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Find 10 coffee with highest quality-price ratio\n",
        "#which means higher rating per unit price\n",
        "\n",
        "# Use the `withColumn` method to calculate the \"value-for-money\" index and add it as a new column to the DataFrame\n",
        "dfs = dfs.withColumn(\"value_for_money\", dfs.rating / dfs[\"100g_USD\"])\n",
        "\n",
        "# Sort the DataFrame based on the \"value_for_money\" index in descending order\n",
        "sorted_df = dfs.sort(dfs.value_for_money.desc())\n",
        "\n",
        "# Display the \"name\" and \"value_for_money\" columns from the sorted DataFrame\n",
        "sorted_df.select(\"name\", \"value_for_money\").show(10)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "fd75362e27048f1ead3b65beb4812b1da3d387150557ce53b099093c32022a5e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
