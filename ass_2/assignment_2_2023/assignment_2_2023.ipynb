{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vl2keC7qQ1B8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0S3PBIZQ2NH"
      },
      "source": [
        "<h1><center>Big Data Algorithms Techniques & Platforms</center></h1>\n",
        "<h2>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "<center>Spark and DataFrames</center>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g53MVOhDv7Oe"
      },
      "source": [
        "#A. Analysis of the ``Great Expectations`` - Charles Dickens\n",
        "\n",
        "Suppose you have a file containing the text of the ``Great Expectation``, a novel written in English by Charles Dickens. You can see an excerpt below :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwiHRqBNv8Tf"
      },
      "source": [
        "\"I pointed to where our village lay, on the flat in-shore among the\n",
        "alder-trees and pollards, a mile or more from the church.\n",
        "\n",
        "The man, after looking at me for a moment, turned me upside down, and\n",
        "emptied my pockets. There was nothing in them but a piece of bread.\n",
        "When the church came to itself,—for he was so sudden and strong that he\n",
        "made it go head over heels before me, and I saw the steeple under my\n",
        "feet,—when the church came to itself, I say, I was seated on a high\n",
        "tombstone, trembling while he ate the bread ravenously.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI8DIAIBdPQ7"
      },
      "source": [
        "## <strong>Exercise 0.</strong> Support functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bRoe2rJxdIvk"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/spark-3.5.0-bin-hadoop3/./bin/spark-submit'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [5], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m findspark\u001b[38;5;241m.\u001b[39minit()\n\u001b[1;32m     16\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext(conf \u001b[38;5;241m=\u001b[39m conf)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitialization successful!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#read file\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[1;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/java_gateway.py:97\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         signal\u001b[39m.\u001b[39msignal(signal\u001b[39m.\u001b[39mSIGINT, signal\u001b[39m.\u001b[39mSIG_IGN)\n\u001b[1;32m     96\u001b[0m     popen_kwargs[\u001b[39m\"\u001b[39m\u001b[39mpreexec_fn\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m preexec_func\n\u001b[0;32m---> 97\u001b[0m     proc \u001b[39m=\u001b[39m Popen(command, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpopen_kwargs)\n\u001b[1;32m     98\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     \u001b[39m# preexec_fn not supported on Windows\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     proc \u001b[39m=\u001b[39m Popen(command, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpopen_kwargs)\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/subprocess.py:854\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[1;32m    851\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[1;32m    852\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m--> 854\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m    855\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m    856\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m    857\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m    858\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m    859\u001b[0m                         errread, errwrite,\n\u001b[1;32m    860\u001b[0m                         restore_signals, start_new_session)\n\u001b[1;32m    861\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    862\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m    863\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/subprocess.py:1702\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1700\u001b[0m     \u001b[39mif\u001b[39;00m errno_num \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1701\u001b[0m         err_msg \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1702\u001b[0m     \u001b[39mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1703\u001b[0m \u001b[39mraise\u001b[39;00m child_exception_type(err_msg)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/spark-3.5.0-bin-hadoop3/./bin/spark-submit'"
          ]
        }
      ],
      "source": [
        "### Write here all the import function and the support function you need for processing the text\n",
        "### Write here all the import function and the support function you need for processing the text\n",
        "import os\n",
        "import re\n",
        "\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark.sql import functions as F\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "from pyspark.sql.functions import explode, length, split, avg, col, min\n",
        "\n",
        "#Initialization\n",
        "findspark.init()\n",
        "conf = SparkConf().setMaster(\"local\")\n",
        "sc = SparkContext(conf = conf)\n",
        "print(\"initialization successful!\")\n",
        "\n",
        "#read file\n",
        "filepath = '/content/great_expectations.txt'\n",
        "\n",
        "with open(filepath, 'r', encoding='utf-8') as file:\n",
        "    Great_Expectation = file.read()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jeq1OBoKc7aS"
      },
      "source": [
        "## <strong>Exercise 1</strong> Word number in sentences\n",
        "\n",
        "The length of a sentence is the number of words that compose the sentence.\n",
        "\n",
        "Write and comment on the set of Spark operations that return how many sentences we have for each available length in the text. You must also show the five most common lengths.\n",
        "\n",
        "Notice that in the available text, a sentence is a set of lines that ends with a strong punctuation mark (i.e., \".\", \"!\", \"?\", etc.).\n",
        "\n",
        "Notice that:\n",
        "\n",
        "* The novel starts after the <code> *** START OF THE PROJECT GUTENBERG EBOOK GREAT EXPECTATIONS ***</code>\n",
        "\n",
        "* <code> [Illustration] </code> is not a sentence\n",
        "\n",
        "* <code>Chapter VIII.</code> is not a sentence\n",
        "\n",
        "You can introduce your constraints for the parsing. Multiple solutions and points of view are correct. You must comment on your point of view (i.e., the definition of ``sentence'' in your analysis).\n",
        "\n",
        "You can choose if you are considering the stop-words in the count: add your point of view in the comment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg6DHjA5daP1"
      },
      "outputs": [],
      "source": [
        "############## WRITE YOUR CODE HERE ##############\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldd9xwUNdGGF"
      },
      "source": [
        "## <strong>Exercise 2.</strong> The average length of sentences in the entire novel\n",
        "Write and comment on the set of Spark operations that returns and shows the average length of the sentences in the novel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpfbBE_KlPe1"
      },
      "outputs": [],
      "source": [
        "############## WRITE YOUR CODE HERE ##############\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9qdDteilCiO"
      },
      "source": [
        "## <strong>Exercise 3.</strong> Average length in chapters\n",
        "\n",
        "Write and comment on the set of Spark operations that returns and shows the average length of sentences in each chapter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48F6wdNBkADi"
      },
      "outputs": [],
      "source": [
        "############## WRITE YOUR CODE HERE ##############\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgxgDiJlKn8s"
      },
      "source": [
        "## <strong>Exercise 4.</strong> The most repeated words in the sentences\n",
        "\n",
        "Write and comment on the set of Spark operations that returns the ten most repeated words in a sentence and their repetition rate.\n",
        "\n",
        "For example, the most repeated word in this set of sentences is <code> cat </code>, with an average repetition rate of 2. The word <code> table </code> is not considered as repeated.\n",
        "\n",
        "* The cat is on the cat table.\n",
        "* The table is red.\n",
        "* The wooden table is broken.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCNmkkXHKmNU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw2bvV93KmAW"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a0nrGmRQ7Yk"
      },
      "source": [
        "# B. Coffee Dataset\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "For running this series of exercises, we are going to use a dataset coming from <a href=\"https://www.kaggle.com/datasets/schmoyote/coffee-reviews-dataset\">Kaggle</a>.\n",
        "\n",
        "As stated in the description of the dataset:\n",
        "\"Dataset contains information about coffee production and consumption.\n",
        "\n",
        "All data are available from the official ICO website:\n",
        "https://www.ico.org/new_historical.asp\".\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "### The dataset\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "The dataset is in a .csv file, and among the columns, you can find:\n",
        "\n",
        "<ul>\n",
        "<li> <code>name</code> The name of the blend </li>\n",
        "<li><code>roaster</code> The name of the roaster</li>\n",
        "<li><code>roast</code> The type of roast</li>\n",
        "<li><code>loc_country</code> The country of the roaster</li>\n",
        "<li><code>...</code> ...</li>\n",
        "</ul>\n",
        "</p>\n",
        "</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-FdlIK_9tT3"
      },
      "outputs": [],
      "source": [
        "! pip install kaggle\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CThDJ818ANV"
      },
      "outputs": [],
      "source": [
        "! kaggle datasets download schmoyote/coffee-reviews-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJ-P2NQd9uKI"
      },
      "outputs": [],
      "source": [
        "!unzip coffee-reviews-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDob14Yt8MLQ"
      },
      "outputs": [],
      "source": [
        "# Add your imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr5auOga9X0Q"
      },
      "source": [
        "## <strong> Spark and Pandas.</strong>\n",
        "\n",
        "For this set of exercises, you must import data in Spark. After this first import, you can pass any dataset to Pandas for data analysis. At the end of each exercise (when the question is pertinent), you must return (reconvert) the dataframe in Spark.\n",
        "\n",
        "Each time you do this conversion you must comment about this. Example:\n",
        "\n",
        "<code> # creating a Spark dataframe </code>\n",
        "\n",
        "<code> df = ... </code>\n",
        "\n",
        "<code> # using Pandas and creating a Pandas dataframe </code>\n",
        "\n",
        "<code> dfp = df. ... </code>\n",
        "\n",
        "<code> # back to Spark </code>\n",
        "\n",
        "<code> dfs = ... </code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBNt1hUkTGUQ"
      },
      "source": [
        "## <strong> Exercise 5.</strong> First import and data type\n",
        "Import the .csv file in Spark DataFrame and show the structure of the dataframe.\n",
        "\n",
        "Check and comment about the data type of each column. As you know, a good data analysis always starts from understanding your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM5Oi6vMR5OC"
      },
      "outputs": [],
      "source": [
        "# Write the command that creates (reads) a Spark DataFrame and stores the reference in the dfs variable\n",
        "\n",
        "#'''############## WRITE HERE YOUR CODE ##############'''\n",
        "dfs =\n",
        "\n",
        "# show the DataFrame schema\n",
        "dfs\n",
        "\n",
        "###### Write here your comment ##############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ1400g4h7fK"
      },
      "source": [
        "## <strong> Exercise 6.</strong> Data modeling choices\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Comment on the data-modeling choices and if you consider them correct from a general data-modeling point of view.\n",
        "</font>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I80ZGgPGtUd6"
      },
      "outputs": [],
      "source": [
        "###### Write here your comment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET9gY8MH21Ew"
      },
      "source": [
        "## <strong> Exercise 7.</strong>  Best rated coffees\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "We want to find the 5 best rated coffees.\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdVft7Fq3pPe"
      },
      "outputs": [],
      "source": [
        "############## WRITE YOUR CODE HERE ##############\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e-mami838aZ"
      },
      "source": [
        "## <strong> Exercise 8.</strong> The best 10 roasters\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "We want to find the ten best roasters. It is up to you to define and refine the ``best'' metric (best in the platform from the beginning of data collection, best in the last three years, best and with a minimum number of ratings, etc.).\n",
        "\n",
        "Add the definition of your best metric in the comments.\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68a38QpWvbqZ"
      },
      "outputs": [],
      "source": [
        "############## WRITE YOUR CODE HERE ##############\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWyhAOgxvdp2"
      },
      "source": [
        "## <strong> Exercise 9.</strong> Best country\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "If you were a roaster, in which three countries would you try to set your business?\n",
        "\n",
        "Show them and refine your metric definition if necessary.\n",
        "</font>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcdMj2NlwOZl"
      },
      "outputs": [],
      "source": [
        "############## WRITE YOUR CODE HERE AND THE DESCRIPTION OF YOUR \"SOMEHOW\" ##############\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IagcQAfBAC-7"
      },
      "source": [
        "## <strong> Exercise 10.</strong> Less common origin\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Show the ten less common origins of the coffees.\n",
        "\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmnpTOAvAD1N"
      },
      "outputs": [],
      "source": [
        "############## WRITE YOUR CODE HERE AND THE DESCRIPTION OF YOUR \"SOMEHOW\" ##############\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wpmj23nwTDk"
      },
      "source": [
        "## <strong> Exercise 11.</strong>  Propose your own analysis\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Propose here an analysis on the dataframe.\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rb-wdPJEwcLF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "fd75362e27048f1ead3b65beb4812b1da3d387150557ce53b099093c32022a5e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
